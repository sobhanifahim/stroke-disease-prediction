# -*- coding: utf-8 -*-
"""stroke_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J_orRXQaRP73U-ul5MNFHwCzKE4lJCtr
"""

import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
import scipy.stats as stats
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve
from PIL import Image
from sklearn.tree import export_graphviz
from IPython.display import Image
from sklearn import tree

df=pd.read_csv('/content/drive/MyDrive/healthcare-dataset-stroke-data.csv')
df

df.drop('id',axis=1,inplace=True)
print(df.shape)
df.head()

categorical_value=['gender','ever_married','work_type','Residence_type','smoking_status']
for i in categorical_value:
  print('\nAttribute: %s\n'%i)
  print(df[i].value_counts())

df.drop(df[df.gender == 'Other'].index, axis = 0, inplace=True)
df_cp = df.copy()
dmy_df=pd.get_dummies(df.iloc[:,[0,4,6]],drop_first=True)
df['gender'] = dmy_df.iloc[:, 0]
df['ever_married'] = dmy_df.iloc[:, 1]
df['Residence_type'] = dmy_df.iloc[:, 2]


freq_smoking = (df.groupby('smoking_status').size()) / len(df)
freq_smoking['Unknown'] = 0 
df['smoking_status'] = df['smoking_status'].apply(lambda x : freq_smoking[x])

freq_work = (df.groupby('work_type').size()) / len(df)
df['work_type'] = df['work_type'].apply(lambda x : freq_work[x])

df.head(20)

print("age")
upper_quartile_age = df["age"].mean() + 3*df["age"].std()
lower_quartile_age = df["age"].mean() - 3*df["age"].std()
print("Upper bound: ", upper_quartile_age)
print("Lower bound: ", lower_quartile_age)
outliers_age = df[(df["age"] > upper_quartile_age) | (df["age"] < lower_quartile_age)]
print('\nTotal Outlier for age = ',outliers_age.shape[0],'\n')

print("avg_glucose_level")
upper_quartile_glucose = df["avg_glucose_level"].mean() + 3*df["avg_glucose_level"].std()
lower_quartile_glucose = df["avg_glucose_level"].mean() - 3*df["avg_glucose_level"].std()
print("Upper bound:",upper_quartile_glucose)
print("Lower bound:", lower_quartile_glucose)
outliers_glucose = df[(df["avg_glucose_level"] > upper_quartile_glucose) | (df["avg_glucose_level"] < lower_quartile_glucose)]
print('\nTotal Outlier for avg_glucose_level = ',outliers_glucose.shape[0],'\n')

print("bmi")
upper_quartile_bmi = df["bmi"].mean() + 3*df["bmi"].std()
lower_quartile_bmi = df["bmi"].mean() - 3*df["bmi"].std()
print("Upper bound: ", upper_quartile_bmi)
print("Lower bound: ", lower_quartile_bmi)
outliers_bmi = df[(df["bmi"] > upper_quartile_bmi) | (df["bmi"] < lower_quartile_bmi)]
print('\nTotal Outlier for bmi = ',outliers_bmi.shape[0],'\n')

df1 = pd.DataFrame(data=df, columns=['age','avg_glucose_level', 'bmi'])
boxplot = sns.boxplot(x="variable", y="value", data=pd.melt(df1))

outliers = pd.concat([outliers_glucose,outliers_bmi]).drop_duplicates()
df.drop(outliers.index, axis=0, inplace=True)

scaler = MinMaxScaler()
df[["age", "avg_glucose_level", "bmi"]] = scaler.fit_transform(df[["age", "avg_glucose_level", "bmi"]])
df.head()

for i in range(df.shape[1]):
    n_missing_data = df.iloc[:, i].isnull().sum()
    percentage = n_missing_data / df.shape[0] * 100
    print('> %s, Missing data: %d (%.1f%%)' % (df.columns[i], n_missing_data, percentage))

print('Checking wheather we can drop the value or have to fill the value')

df.loc[df.stroke == 1].shape[0]

df.loc[(df.bmi.isna() == True) & (df.stroke == 1)].shape[0]

imputer = KNNImputer(n_neighbors=5)
df = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)
df.isna().any()

df.head()

df.hist(bins=50,figsize=(20,10),alpha=0.6)
plt.show()

X1 = df.drop('stroke', axis=1)
Y1 = df.iloc[:, -1]
X2 = df.drop('stroke', axis=1)
Y2 = df.iloc[:, -1]
X3 = df.drop('stroke', axis=1)
Y3 = df.iloc[:, -1]
X4 = df.drop('stroke', axis=1)
Y4 = df.iloc[:, -1]
X5 = df.drop('stroke', axis=1)
Y5 = df.iloc[:, -1]
X6 = df.drop('stroke', axis=1)
Y6 = df.iloc[:, -1]
X7 = df.drop('stroke', axis=1)
Y7 = df.iloc[:, -1]

X_train, X_test, y_train, y_test = train_test_split(X1, Y1, test_size=0.25, random_state=77)
print(len(X_train))
print(len(X_test))

dt = DecisionTreeClassifier()

dt.fit(X_train,y_train)

decision_tree=tree.export_graphviz(dt,out_file='tree.dot',feature_names=X_train.columns,filled=True)

!dot -Tpng tree.dot -o tree.png

image=plt.imread('tree.png')
plt.figure(figsize=(200,500))
plt.imshow(image)

y_pred=dt.predict(X_test)

y_pred

score1=accuracy_score(y_test,y_pred)*100
print("Accuracy using decision tree: ", round(score1,1),"%")

rf= RandomForestClassifier (n_estimators=10, max_features="auto", random_state=77)

X1_train, X1_test, y1_train, y1_test = train_test_split(X2, Y2, test_size=0.25, random_state=77)
print(len(X1_train))
print(len(X1_test))

rf.fit(X1_train,y1_train)

y1_pred=rf.predict(X1_test)

score2=accuracy_score(y1_test,y1_pred)*100
print("Accuracy using random forest: ", round(score2,1),"%")

knn = KNeighborsClassifier(n_neighbors=80)

X2_train, X2_test, y2_train, y2_test = train_test_split(X3, Y3, test_size=0.25, random_state=77)
print(len(X2_train))
print(len(X2_test))

knn.fit(X2_train,y2_train)

y2_pred=dt.predict(X2_test)

score3=accuracy_score(y2_test,y2_pred)*100
print("Accuracy using KNeighbours: ", round(score3,1),"%")

Lr = LogisticRegression(random_state=20)

X3_train, X3_test, y3_train, y3_test = train_test_split(X4, Y4, test_size=0.25, random_state=77)
print(len(X3_train))
print(len(X3_test))

Lr.fit(X3_train,y3_train)

y3_pred=dt.predict(X3_test)

score4=accuracy_score(y3_test,y3_pred)*100
print("Accuracy using Logistic Regression: ", round(score4,1),"%")

svm = SVC()

X4_train, X4_test, y4_train, y4_test = train_test_split(X5, Y5, test_size=0.25, random_state=77)
print(len(X4_train))
print(len(X4_test))

svm.fit(X4_train,y4_train)

y4_pred=dt.predict(X4_test)

score5=accuracy_score(y4_test,y4_pred)*100
print("Accuracy using SVM: ", round(score5,1),"%")

AdaB=AdaBoostClassifier()

X5_train, X5_test, y5_train, y5_test = train_test_split(X6, Y6, test_size=0.25, random_state=77)
print(len(X5_train))
print(len(X5_test))

AdaB.fit(X5_train,y5_train)

y5_pred=dt.predict(X5_test)

score6=accuracy_score(y5_test,y5_pred)*100
print("Accuracy using AdaBoost: ", round(score6,1),"%")

Gd=GradientBoostingClassifier()

X6_train, X6_test, y6_train, y6_test = train_test_split(X7, Y7, test_size=0.25, random_state=77)
print(len(X6_train))
print(len(X6_test))

Gd.fit(X6_train,y6_train)

y6_pred=dt.predict(X6_test)

score7=accuracy_score(y6_test,y6_pred)*100
print("Accuracy using Gradient Boosting: ", round(score7,1),"%")

inp=(0.0,	0.975586,	1.0,	0.0,	1.0,	0.572323,	1.0,	0.154140,	0.449048,	0.370327)
inpdata=np.asarray(inp)
reshaped=inpdata.reshape(1,-1)
pred=rf.predict(reshaped)
print(pred)